{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numpy.linalg import norm\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Feature Extraction and Clustering\n",
    "\n",
    "Term frequency - inverse document frequency (tf-idf) is a method to evaluate how important is a word in a document. It is a model to transform textual representation of information into a vector-space model (VSM)\n",
    "\n",
    "VSM is a model representing textual information as a vector which:\n",
    "1.  could represent the importance of a term (tf–idf) or even the absence or;\n",
    "2. represent presence (Bag of Words) of a term in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency - Inverse Document Frequency (tf-idf)\n",
    "\n",
    "The **term frequency** of a document $d$ with respect a term $t$ is defined as:\n",
    "$$\n",
    "    \\text{tf}(t, d) = \\sum_{\\hat t \\in d}\\mathbb{1}_{\\hat t = t}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       "        [0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0],\n",
       "        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec = CountVectorizer()\n",
    "# Dense representation: |D| x F\n",
    "train_docs = [\"this is a test\", \"this is another test\", \"that is the final test\", \"what is this about?\"]\n",
    "train_docs = [\"esta es una prueba\", \"esta es otra prueba\", \"esa fue la prueba final\", \"¿De qué es esto?\"]\n",
    "Mtf = count_vec.fit_transform(train_docs)\n",
    "Mtf.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'esta': 3,\n",
       " 'es': 1,\n",
       " 'una': 11,\n",
       " 'prueba': 9,\n",
       " 'otra': 8,\n",
       " 'esa': 2,\n",
       " 'fue': 6,\n",
       " 'la': 7,\n",
       " 'final': 5,\n",
       " 'de': 0,\n",
       " 'qué': 10,\n",
       " 'esto': 4}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem with a a term frequency model is that is scales up frequent terms and scales down rare terms, which may be more informative than higher frequency terms. **Term frequency - inverse document frequency**, or tf-id, attempts to solve that by logarithmically scaling up rare terms and scaling down higher order terms.\n",
    "\n",
    "We define the inverse document frequency (**idf**) as:\n",
    "$$\n",
    "    \\text{idf}(t) = \\log \\frac{|D|}{1 + |\\{d:  t \\in \\ d\\}|}\n",
    "$$\n",
    "\n",
    "Where $|D|$ is the number of documents in the corpus and, $|\\{d:  t \\in \\ d\\}|$ is the number of documents containing the term $t$\n",
    "\n",
    "Finally, the tf-idf formula is defined as:\n",
    "\n",
    "$$\n",
    "    \\text{tf-idf}(t) = \\text{df}(t, d) \\times \\text{idf}(t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esta\t1.51\n",
      "es\t1.00\n",
      "una\t1.92\n",
      "prueba\t1.22\n",
      "otra\t1.92\n",
      "esa\t1.92\n",
      "fue\t1.92\n",
      "la\t1.92\n",
      "final\t1.92\n",
      "de\t1.92\n",
      "qué\t1.92\n",
      "esto\t1.92\n"
     ]
    }
   ],
   "source": [
    "card_D = len(train_docs)\n",
    "idf = np.log((1 + card_D) / np.array([1 + np.sum([v in doc for doc in train_docs]) for v in count_vec.vocabulary_])) + 1\n",
    "\n",
    "for term, weight in zip(count_vec.vocabulary_, idf):\n",
    "    print(f\"{term}\\t{weight:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix $\\text{tf-idf}$ is defined as\n",
    "$$\n",
    "    M_{\\text{tf-idf}} = M_{tf} \\times M_{idf}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    0.319 0.    0.39  0.    0.    0.    0.    0.    0.611 0.    0.611]\n",
      " [0.    0.319 0.    0.39  0.    0.    0.    0.    0.611 0.611 0.    0.   ]\n",
      " [0.    0.    0.447 0.    0.    0.447 0.447 0.447 0.    0.447 0.    0.   ]\n",
      " [0.463 0.307 0.    0.    0.588 0.    0.    0.    0.    0.    0.588 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "Mtf_idf = Mtf @ np.identity(idf.shape[0]) *  idf\n",
    "# L2 normalizatoin\n",
    "Mtf_idf = Mtf_idf / norm(Mtf_idf, ord=2, axis=1).reshape(-1, 1)\n",
    "print(Mtf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine the steps above using scikit-learn's `TfidfTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    0.409 0.    0.505 0.    0.    0.    0.    0.    0.409 0.    0.641]\n",
      " [0.    0.409 0.    0.505 0.    0.    0.    0.    0.641 0.409 0.    0.   ]\n",
      " [0.    0.    0.476 0.    0.    0.476 0.476 0.476 0.    0.304 0.    0.   ]\n",
      " [0.542 0.346 0.    0.    0.542 0.    0.    0.    0.    0.    0.542 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_trans = TfidfTransformer(norm=\"l2\")\n",
    "Mtf_idf = tfidf_trans.fit_transform(Mtf)\n",
    "print(Mtf_idf.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can make use of `CountVectorizer` and `TfidfTransformer` in one step using `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    0.409 0.    0.505 0.    0.    0.    0.    0.    0.409 0.    0.641]\n",
      " [0.    0.409 0.    0.505 0.    0.    0.    0.    0.641 0.409 0.    0.   ]\n",
      " [0.    0.    0.476 0.    0.    0.476 0.476 0.476 0.    0.304 0.    0.   ]\n",
      " [0.542 0.346 0.    0.    0.542 0.    0.    0.    0.    0.    0.542 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "tfv = TfidfVectorizer(norm=\"l2\", smooth_idf=True)\n",
    "tfv_matrix = tfv.fit_transform(train_docs)\n",
    "print(tfv_matrix.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to measure the distance between two documents we make use of the cosine similarity\n",
    "\n",
    "$$\n",
    "    \\cos \\theta = \\frac{a\\cdot b}{||a||_2\\cdot||b||_2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.   , 0.59 , 0.124, 0.141],\n",
       "       [0.59 , 1.   , 0.124, 0.141],\n",
       "       [0.124, 0.124, 1.   , 0.   ],\n",
       "       [0.141, 0.141, 0.   , 1.   ]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(tfv_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1.   , 0.59 , 0.124, 0.141],\n",
       "        [0.59 , 1.   , 0.124, 0.141],\n",
       "        [0.124, 0.124, 1.   , 0.   ],\n",
       "        [0.141, 0.141, 0.   , 1.   ]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since each row is already normalize, Mtfv @ Mtfv.T is equivalent to the\n",
    "# dot product between any two documents\n",
    "(tfv_matrix @ tfv_matrix.T).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization \n",
    "\n",
    "* *Stemming* usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.\n",
    "* *Lemmatization* usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def spanish_stemmer(corpus, lang=\"spanish\"):\n",
    "    stemmer = SnowballStemmer(lang)\n",
    "    corpus_tokens = word_tokenize(re.sub(\"[^\\w\\s]\", \"\", corpus).lower(), language=lang)\n",
    "    return [stemmer.stem(w) for w in corpus_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['los', 'indic', 'dispon', 'al', 'segund', 'trimestr', 'de', '2018', 'sugier', 'que', 'la', 'econom', 'mundial', 'continu', 'expand', 'a', 'un', 'ritm', 'relat', 'elev']\n"
     ]
    }
   ],
   "source": [
    "test_text = (\"Los indicadores disponibles al segundo trimestre de 2018 sugieren que la \"\n",
    "             \"economía mundial continuó expandiéndose a un ritmo relativamente elevado\")\n",
    "print(spanish_stemmer(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    0.409 0.    0.505 0.    0.    0.    0.    0.    0.409 0.    0.641]\n",
      " [0.    0.409 0.    0.505 0.    0.    0.    0.    0.641 0.409 0.    0.   ]\n",
      " [0.    0.    0.476 0.    0.    0.476 0.476 0.476 0.    0.304 0.    0.   ]\n",
      " [0.542 0.346 0.    0.    0.542 0.    0.    0.    0.    0.    0.542 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "tf = TfidfVectorizer(tokenizer=spanish_stemmer)\n",
    "m_tfidf = tf.fit_transform(train_docs)\n",
    "print(m_tfidf.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.    0.59  0.124 0.141]\n",
      " [0.59  1.    0.124 0.141]\n",
      " [0.124 0.124 1.    0.   ]\n",
      " [0.141 0.141 0.    1.   ]]\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = cosine_similarity(m_tfidf)\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randn, multivariate_normal\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = multivariate_normal([10, 0], [[1, 0], [0, 1]], size=100)\n",
    "c2 = multivariate_normal([0, 10], [[1, 0], [0, 1]], size=100)\n",
    "clust = np.r_[c1, c2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a1fbb6f60>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.scatter(*clust.T, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9949192988798035\n"
     ]
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import cophenet, linkage\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "clustering = linkage(similarity_matrix, metric=\"cosine\")\n",
    "c, _  = cophenet(clustering, pdist(similarity_matrix))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row resulting from the `linkage` function will have the following form:\n",
    "> `[idx1, idx2, distance, sample_count]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.   , 1.   , 0.122, 2.   ],\n",
       "       [3.   , 4.   , 0.695, 3.   ],\n",
       "       [2.   , 5.   , 0.73 , 4.   ]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1. http://blog.christianperone.com/2011/09/machine-learning-text-feature-extraction-tf-idf-part-i/\n",
    "2. https://sites.temple.edu/tudsc/2017/03/30/measuring-similarity-between-texts-in-python/\n",
    "3. https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "4. https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
